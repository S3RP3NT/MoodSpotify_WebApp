{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import imutils\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "from scipy.spatial import distance\n",
    "from scipy import ndimage\n",
    "from scipy import stats\n",
    "from imutils import face_utils\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables\n",
    "shape_x = 48\n",
    "shape_y = 48\n",
    "input_shape = (shape_x, shape_y, 1)\n",
    "nClasses = 7\n",
    "thresh = 0.25\n",
    "frame_check = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "        A = distance.euclidean(eye[1], eye[5])\n",
    "        B = distance.euclidean(eye[2], eye[4])\n",
    "        C = distance.euclidean(eye[0], eye[3])\n",
    "        ear = (A + B) / (2.0 * C)\n",
    "        return ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(frame):\n",
    "        #Cascade classifier pre-trained model\n",
    "        cascPath = 'Models/face_landmarks.dat'\n",
    "        faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "        #BGR -> Gray conversion\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        #Cascade MultiScale classifier\n",
    "        detected_faces = faceCascade.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=6,\n",
    "                                                      minSize=(shape_x, shape_y),\n",
    "                                                      flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        coord = []\n",
    "                                              \n",
    "        for x, y, w, h in detected_faces :\n",
    "            if w > 100 :\n",
    "                sub_img=frame[y:y+h,x:x+w]\n",
    "                cv2.rectangle(frame,(x,y),(x+w,y+h),(0, 255,255),1)\n",
    "                coord.append([x,y,w,h])\n",
    "\n",
    "        return gray, detected_faces, coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_features(faces, offset_coefficients=(0.075, 0.05)):\n",
    "        gray = faces[0]\n",
    "        detected_face = faces[1]\n",
    "        \n",
    "        new_face = []\n",
    "        \n",
    "        for det in detected_face :\n",
    "            x, y, w, h = det\n",
    "            horizontal_offset = np.int(np.floor(offset_coefficients[0] * w))\n",
    "            vertical_offset = np.int(np.floor(offset_coefficients[1] * h))\n",
    "            extracted_face = gray[y+vertical_offset:y+h, x+horizontal_offset:x-horizontal_offset+w]\n",
    "            new_extracted_face = zoom(extracted_face, (shape_x / extracted_face.shape[0],shape_y / extracted_face.shape[1]))\n",
    "            new_extracted_face = new_extracted_face.astype(np.float32)\n",
    "            new_extracted_face /= float(new_extracted_face.max())\n",
    "            new_face.append(new_extracted_face)\n",
    "        return new_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catchEmotion():\n",
    "    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "    (nStart, nEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"nose\"]\n",
    "    (mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"mouth\"]\n",
    "    (jStart, jEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"jaw\"]\n",
    "    (eblStart, eblEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eyebrow\"]\n",
    "    (ebrStart, ebrEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eyebrow\"]\n",
    "    model = load_model('Models/video.h5', compile=False)\n",
    "    optimizer = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    face_detect = dlib.get_frontal_face_detector()\n",
    "    predictor_landmarks  = dlib.shape_predictor(\"Models/face_landmarks.dat\")\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    final_result = []\n",
    "    counter = 10\n",
    "    while True:\n",
    "        if(counter<=0):\n",
    "            break\n",
    "        ret, frame = video_capture.read()\n",
    "        face_index = 0\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rects = face_detect(gray, 1)\n",
    "        for (i, rect) in enumerate(rects):\n",
    "            shape = predictor_landmarks(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "            (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "            face = gray[y:y+h,x:x+w]\n",
    "            face = zoom(face, (shape_x / face.shape[0],shape_y / face.shape[1]))\n",
    "            face = face.astype(np.float32)\n",
    "            face /= float(face.max())\n",
    "            face = np.reshape(face.flatten(), (1, 48, 48, 1))\n",
    "            prediction = model.predict(face)\n",
    "            prediction_result = np.argmax(prediction)\n",
    "            if(prediction_result>0):\n",
    "                counter = counter - 1\n",
    "                final_result.append(prediction_result) \n",
    "    final = np.array(final_result)\n",
    "    result = int(stats.mode(final)[0])\n",
    "    emotion = {0:'Angry',1:'Disgust',2:'Fear',3:'Happy',4:'Sad',5:'Surprise',6:'Neutral'}\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return emotion[result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sad\n"
     ]
    }
   ],
   "source": [
    "print(catchEmotion())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
